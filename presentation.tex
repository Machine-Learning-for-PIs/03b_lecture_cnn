% no notes
\documentclass{beamer}
% notes and slides
%\documentclass[notes]{beamer}
% notes only
%\documentclass[notes=only]{beamer}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multirow}
\usepackage{multimedia}
\usepackage{circuitikz}
\usepackage{epigraph}
\usepackage{url}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{tikz}
\usetikzlibrary{patterns,shapes.arrows}
\usetikzlibrary{positioning}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots,dateplot}
\usepackage{standalone}
\usepackage{adjustbox}
\usepackage{lmodern}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage{standalone}
\usepackage{caption}
\usepackage{csquotes}

% python listings
\input{python.tex}

\definecolor{cyan}{RGB}{42,161,152}
\definecolor{violet}{RGB}{108,113,196}
\definecolor{red}{RGB}{220,50,47}

\PassOptionsToPackage{american}{babel} % change this to your language(s), main language last
% Spanish languages need extra options in order to work with this template
% \PassOptionsToPackage{spanish,es-lcroman}{babel}
\usepackage{babel}

\PassOptionsToPackage{%
  backend=biber,bibencoding=utf8, %instead of bibtex
  %backend=bibtex8,bibencoding=ascii,%
  language=auto,%
  style=numeric-comp,%
  %style=authoryear-comp, % Author 1999, 2010
  %bibstyle=authoryear,dashed=false, % dashed: substitute rep. author with ---
  style=alphabetic,
  sorting=nyt, % name, year, title
  maxbibnames=10, % default: 3, et al.
  %backref=true,%
  %natbib=true % natbib compatibility mode (\citep and \citet still work)
}{biblatex}
\usepackage{biblatex}
\input{macros.tex}
% Redefine the caption format to remove "Figure"
\captionsetup[figure]{labelformat=empty}

\newcommand{\referencefootnote}[1]{\setbeamertemplate{footline}[text line]{%%%
\parbox{0.9\paperwidth}{\vspace*{-23pt}\tiny{\textcolor{gray}{#1}}\hfill\scriptsize\insertframenumber}}}

\addbibresource{bib.bib}

\usetheme{metropolis}           % Use metropolis theme
\setbeamertemplate{caption}[default]
\title{Introduction to Convolutional Neural Networks}
\date{February 24, 2025}%{\today}
%\institute{High-Performance Computing and Analytics Lab}
%\author{Moritz Wolter}
\institute{Visual Computing Group, University of Bonn}
\author{Elena Trunz}

\titlegraphic{\includegraphics[width=2.00cm]{UNI_Bonn_Logo_Standard_RZ.pdf}}
\begin{document}
    \maketitle

    \begin{frame}
    \frametitle{Overview} 
    \tableofcontents
    \end{frame}

    %\begin{frame}{Motivation \cite{goodfellow2016deep}}
        %\begin{itemize}
            %\item sparse interactions
            %\item parameter sharing
            %\item equivariant representations (i.e. with respect to translation)
            %\item efficiency
            %\item Train deeper networks.
        %\end{itemize}
        %\note{
            %Sparse interactions: From dense to block circulant matrix. \\
            %Parameter sharing: Use the same parameters for more than one job. \\
            %Equvariance: Translations of an input should not change the outcome.
        %}
    %\end{frame}
		
		\section{Why Convolution?}
		\begin{frame}{Invariance in the network structure}
	\begin{figure}
        \includestandalone[width=0.7\linewidth,height=1.5cm]{./figures/mnist_sequence}
      \end{figure}
			\begin{itemize}
		\item Consider the task of recognizing handwritten digits:
		\begin{itemize}
			\item Input image: set of pixel intensities
			\item Desired output: posterior probability over the ten digits
		\end{itemize} %\pause
		\item A system recognizing digits should be invariant to
		\begin{itemize}
			\item Translations
			\item Scaling
			\item (Small) rotations
			\item Some elastic deformations
		\end{itemize}
		\end{itemize}
	\end{frame}

	\begin{frame}{Fully connected network?}
	\begin{itemize}
		\item Given enough training data a fully connected network solves the task. %\pause
		\item However, we can do better: %\pause
		\begin{itemize}
			\item Pixels near to each other are more correlated to each other than pixels far apart. %\pause
			\item Local features which are useful in one region of the image are likely to be useful elsewhere as well, e.g. for detecting a translated object.
		\end{itemize} %\pause
		%\item In 1989 Yann LeCun proposed to build such invariance properties into the structure of a neural network. This is the basis for convolutional neural networks.
	\end{itemize} 
	\end{frame}

    \begin{frame}{The invention of convolutional neural networks}
        %Proposed in Yann le Cun's \cite{lecun1989handwritten}.
        In 1989 Yann LeCun proposed to build invariance properties into the structure of a neural network. This is the basis for convolutional neural networks.
				\begin{figure}%
				\centering
				\includegraphics[width=.6\linewidth]{figures/IMG_20220923_090433.jpg}
				\end{figure}   
    \end{frame}
		
    \section{Convolution operation in machine learning}

    %\begin{frame}{Defining convolution}
			%For two one-dimensional signals $x \in \mathbb{R}^T$
        %and $k \in \mathbb{R}^T$, convolution is defined as
        %\begin{align}
            %s(t) = (x * k)(t) = \sum_{a=0}^{T} x(a)k(t - a),
        %\end{align}
        %for numbers $t,a$. Possible $t$ will depend on signal length and padding. \\
        %In 2D, we require a kernel matrix $K \in \mathbb{R}^{O,P}$ and a image matrix
        %$I \in \mathbb{K}^{N,M}$
        %\begin{align}
            %S(i,j) = (K * I)(i,j) = \sum_m^M \sum_n^N I(i-m, j-n)K(n,m)
        %\end{align}
        %Again not just any $i,j$ will do. We will see what this means in a minute.
    %\end{frame}
		
		 \begin{frame}{Defining discrete convolution}
		\begin{itemize}
			\item For two one-dimensional signals $x \in \mathbb{R}^T$
        and $k \in \mathbb{R}^T$, convolution is defined as
        \begin{align}
            s(t) = (x * k)(t) = \sum_{m=0}^{T} x(m)k(t - m),
        \end{align}
				where $t$ denote the position in the output sequence. 
        \item In 2D, we require a kernel matrix $K \in \mathbb{R}^{O,P}$ and a image matrix
        $I \in \mathbb{K}^{N,M}$
        \begin{align}
            S(i,j) = (K * I)(i,j) = \sum_m^M \sum_n^N I(i-m, j-n)K(n,m)
        \end{align}
				where $(i,j)$ denote the position in the output image.
				\end{itemize}
				\note{$m$ and $n$ are indices that iterate over the elements of the input and the kernel.}
    \end{frame}


    \begin{frame}{Defining cross-correlation}
        \begin{itemize}
					\item Cross-correlation is convolution without flipping the kernel: % \cite{goodfellow2016deep}
        \begin{align}
            S(i,j) = (K*I)(i,j) = \sum_m^M \sum_n^N I(i+m, j+n)K(m,n)
        \end{align}
        \item Many machine-learning libraries implement cross-correlation and
        call it convolution. 
				\item In this course we will follow their example.
				\end{itemize}
    \note{
        A convolution example on the board: 
        \begin{align}
            \mathbf{I} = \begin{pmatrix}
                1 & 3 & -1 \\
                2 & 1 &  0 \\
                0 & 2 & -1 \\
            \end{pmatrix},
            \mathbf{K} = \begin{pmatrix}
                1 & 0 \\
                2 & -1 \\
            \end{pmatrix}
        \end{align}
        Computing $\mathbf{I}*\mathbf{K}$:
        \begin{align}
            \mathbf{I}*\mathbf{K} &= \begin{pmatrix}
                1\cdot 1 + 3\cdot 0 + 2\cdot 2 + 1\cdot (-1) & 3\cdot 1 + (-1)\cdot 0 + 1\cdot 2 + 0\cdot (-1) \\
                2\cdot 1 + 1\cdot 0 + 0\cdot 2 + 2\cdot (-1) & 1\cdot 1 + 0\cdot 0 + 2\cdot 2 + (-1)\cdot (-1)  
            \end{pmatrix} \\
            &= \begin{pmatrix}
                4 & 5 \\
                0 & 6
            \end{pmatrix}
        \end{align}
    }
    \end{frame}

    \begin{frame}{Convolution}
        \begin{figure}
            \centering
            \includegraphics[scale=0.7]{./figures/no_padding_no_strides_00.pdf}
            \includegraphics[scale=0.7]{./figures/no_padding_no_strides_01.pdf} \\
            \includegraphics[scale=0.7]{./figures/no_padding_no_strides_02.pdf}
            \includegraphics[scale=0.7]{./figures/no_padding_no_strides_03.pdf}
            \caption{Illustration of the convolution operation \cite{dumoulin2016guide}.}
        \end{figure}
    \end{frame}
		
		\begin{frame}{Advantages of convolutions}
	The convolution operation allows us to do this for inputs of varying sizes in a convenient way, and affords advantages of  %\pause 
	\begin{enumerate}
	\item local receptive fields (sparse connectivity limited in range vs. fully connected) %\pause 
	\item parameter sharing (allows for detection of same features all over the image) %\pause 
	\item equivariant representations and subsampling (reduces information hierarchically)
	\end{enumerate}
	\end{frame}


	\begin{frame}{Sparse connectivity and interaction}
	\footnotesize
	\begin{itemize}
		\item In CNNs, the kernel or filter is often much smaller than the input size.  This allows us to process images of varying sizes.  For an input image of millions of pixels we can still detect meaningful features which are local, such as edges. 
		\item A small kernel does not mean that the area of interaction is limited.  For a deep network, the units may \emph{indirectly} interact with a larger portion of the input as information propagates up the layers:
	\captionfig{figures/dl9_4}{0.35}{\footnotesize Unit $g_3$ indirectly interacts with the whole input \cite{goodfellow2016deep}}
	\end{itemize}
	\end{frame}

{ \referencefootnote{Image was taken from \cite{goodfellow2016deep}}
	\begin{frame}{Parameter sharing}
	\footnotesize
	\begin{itemize}
		\item Standard feed-forward network has unit-specific weights. %\pause
		\item Each weight is used only once when computing the activations for subsequent layers. %\pause
		\item In a CNN, the weights of each kernel is applied to every position of the input. %\pause
		\item This \emph{parameter sharing} reduces the amount of storage per model.
		%\captionfig{figures/dl9_5}{0.3}{9.5 from Goodfellow}
		\begin{figure}
					\center
					\includegraphics[scale=.4]{figures/dl9_5.pdf} 
					%\caption{}
					\end{figure}
	\end{itemize}
	\end{frame}
	}
	\note{The black arrows show connections that use a particular parameter set in two different models.  If we were to share parameters, then the single parameter (set) is used at all the input locations.  But if were to have a fully-connected layer, then this set of parameters would be used only once, which is shown in the bottom figure.}


	\begin{frame}{Translation equivariance}
	\begin{itemize}
		\item An \emph{equivariant} function is one where for some given change in the input, the output changes in the same way.%\pause 
		\item With images, convolution creates a 2-D map of where certain features appear in the input. If we move the object in the input, its representation will move the same amount in the output.
	\end{itemize}
	\end{frame}
	\note{equivariance is useful for finding repeating patterns, e.g. line detection;}

	\begin{frame}{Multiple filters / kernels}
	\begin{itemize}
		\item In the context of CNNs, we refer to convolution not in the strict mathematical sense, but an operation that applies many convolutions in parallel.
		\item Convolution with a single kernel extracts only one kind of feature (at many spatial locations).
		\item For each layer, however, we are interested in extracting many kinds of features (at many spatial locations). 
	\end{itemize}
	\end{frame}

	\begin{frame}{Strided convolution}
	\begin{itemize}
		\item To save computational expense, we can skip over some positions of the kernel, i.e. not extract our feature response as finely.
	\begin{figure}
            \centering
            \includegraphics[width=0.25\linewidth]{figures/no_padding_strides_00.pdf}
            \includegraphics[width=0.25\linewidth]{figures/no_padding_strides_01.pdf} \\
            \includegraphics[width=0.25\linewidth]{figures/no_padding_strides_02.pdf}
            \includegraphics[width=0.25\linewidth]{figures/no_padding_strides_03.pdf}
            \caption{Stride two convolutions \cite{dumoulin2016guide}.}
        \end{figure}
				\item[$\rightarrow$ Down-sampling]
		\end{itemize}
	\end{frame}
	
		{ \referencefootnote{Image was taken from \cite{goodfellow2016deep}}
    \begin{frame}{Padded convolution}
		\begin{itemize}
			\item We can control the size of convolution inputs and outputs through padding. 
			\item Without padding, the spatial extent of the network would shrink, with the rate of shrinking directly proportional to the size of the filter or kernel.  
        \begin{figure}
            \centering
            \includegraphics[width=0.45\linewidth]{figures/dl9_13.pdf}
            %\caption{}
        \end{figure}
				\end{itemize}
    \end{frame}
		}

    \begin{frame}{Visualization of padded convolution}
        \begin{figure}
            \centering
            \includegraphics[width=0.25\linewidth]{./figures/full_padding_no_strides_00.pdf}
            \includegraphics[width=0.25\linewidth]{./figures/full_padding_no_strides_01.pdf} \\
            \includegraphics[width=0.25\linewidth]{./figures/full_padding_no_strides_02.pdf}
            \includegraphics[width=0.25\linewidth]{./figures/full_padding_no_strides_03.pdf}
            \caption{Fully padded convolutions with unit strides \cite{dumoulin2016guide}.}
        \end{figure}
    \end{frame}

    \begin{frame}{Summary}
        \begin{itemize}
            \item The convolution operation slides convolution kernels over an image.
            \item Padding avoids losing pixels on the side.
            \item Strided convolutions downsample the input.
            \item Moving in steps of two pixels, for example, cuts the resolution in half.
        \end{itemize}
    \end{frame}

    \section{Understanding convolution}

    \begin{frame}{Getting computers to find Waldo}
        \begin{figure}
            \includegraphics[scale=0.1]{./python/waldo_snow.jpg}
            \includegraphics{./python/waldo_small.jpg}
        \end{figure}
    \end{frame}

    \begin{frame}{Finding Waldo via cross-correlation.}
        \begin{figure}
        \centering
        \includestandalone[scale=1]{./figures/corr_plot}
        %\caption{}
        \end{figure}
    \end{frame}

    \begin{frame}{Summary}
        \begin{itemize}
            \item Cross-correlation is called convolution in the machine learning literature.
            \item Patterns can be located in signals via cross-correlation.
        \end{itemize}
    \end{frame}

    \section{Convolutional neural networks}
    \begin{frame}{Motivating convolutional neural networks (CNN)}
        \begin{itemize}
            \item Fixed filters work if we are looking for a very specific waldo.
            \item In other cases, we need a better solution.
            \item Convolutional neural networks rely on filter optimization via back-propagation.
            \item Filter optimization turns CNNs into very versatile tools!
        \end{itemize}
    \end{frame}

    \begin{frame}{Multichannel convolution}
        \begin{figure}
            \includestandalone[width=.8\linewidth]{./figures/cnn_channels}
            \caption{The plot shows a convolution
            computation using a $3x2x3x3$ kernel on a $2x5x5$ input.
            The kernel pairs convolve with the input, producing $3x3$ results.
            $+$ adds the two channels for each of the three tensors.
            Finally, everything is stacked. Inspired by \cite[page 9]{dumoulin2016guide}. }
        \end{figure}
        \note{
            On the board:
            Explain the effect of the input and output shapes. \\
            I.e.: \\
                Kernel $(O,I,H,W)$:  Out-Channels, In-Channels, Height, Width \\
                Image  $(N,C,H,W)$: Batch-Size, Channels, Height, Width \\
                Results in: \\
                Result $(N,O, H_n, W_n)$ 
        }
    \end{frame}

    \begin{frame}{Computing the output shape of a CNN layer}
        One can determine the output shape for each dimension individually.
        Without zero padding and a stride size of one,
        \begin{align}
            o = (i-k) + 1
        \end{align}
        can be used to compute the output size. $i$ denotes the input size,
        and $k$ is the kernel size. \cite{dumoulin2016guide} covers all cases which appear in practice.
    \end{frame}

    \begin{frame}{Image to column and the forward pass}
        We already know how to train dense network layers using matrix multiplication.
        Training a CNN the same way requires restructuring the image to express convolution as matrix multiplication,
        \begin{align}
            \overline{\mathbf{h}} &= \mathbf{K}_f \mathbf{v}_I  + \mathbf{b}, \\ 
            \mathbf{h}_f &= f(\overline{\mathbf{h}}).
        \end{align}
        $\mathbf{v}_I \in \mathbb{R}$ denotes the restructured image input. $\mathbf{K}_f \in \mathbb{R}^{k_o, k_i \cdot k_h \cdot k_w}$ the flattened restructured kernel.
        $o,i,h,w$ denote the output, input, height, and width dimensions, respectively.
        \note{
            im2col demonstrate on the board:
            Idea: collect the image convolution patches in the columns of a matrix. Use python indexing to set it up.
            For a $3 \times 3$ matrix and a $2 \times 2$ kernel without padding this would lead to the index matrices: 
            \begin{align}
                \begin{pmatrix}
                    0 & 1 & 2 \\
                    3 & 4 & 5 \\
                    6 & 7 & 8 \\
                \end{pmatrix}
                \rightarrow
                \begin{pmatrix}
                0 & 1 & 3 & 4 \\
                1 & 2 & 4 & 5 \\
                3 & 4 & 6 & 7 \\
                4 & 5 & 7 & 8
                \end{pmatrix}
            \end{align}
        }
    \end{frame}

    \begin{frame}{The backward pass}
        We apply the rules for dense layers to the restructured convolutional layer data,
        \begin{align} 
            \delta \mathbf{K}_f &= [f'(\overline{\mathbf{h}}) \odot \triangle]_f \mathbf{v}^T_I,  &  
            \delta \mathbf{b} &= f'(\overline{\mathbf{h}}) \odot \triangle,   \\  
            \delta \mathbf{x} &= \big(\mathbf{K}_f^T [f'(\overline{\mathbf{h}}) \odot \triangle]_f \big)_{I^{-1}}.
        \end{align}
        With $I$ and $I^{-1}$ denoting the \texttt{im2col} and \texttt{col2im} operations.
        All major deep learning frameworks have both operations built in.
    \end{frame}

    \begin{frame}{The classifier at the end}
        \begin{figure}
        \includestandalone[width=\linewidth]{./figures/cnn}
        \caption{The LeNet-architecture\cite{lecun1989handwritten} as illustrated by \cite{StutzCNN}.}
        \end{figure}
    \end{frame}

    \begin{frame}{The shifting input problem}
        \begin{itemize}
            \item With the tools we have seen, shifting an input also shifts the CNN output before the dense classifier.
            \item Shifting the input would shift the input in front of the final dense-classifier neurons.
            \item We want invariance to translation.
        \end{itemize}
    \end{frame}


    \begin{frame}{Pooling}
        Max pooling layers choose maximum values in predefined regions.
        Two by two max pooling, for example, picks the maximum in neighboring areas of four pixels.
        If an input is shifted by two pixels, the result will remain the same!
        Pooling layers are used repeatedly for a cumulative effect.
        \note{
            $\rightarrow $ Draw the effect of max pooling on the board.
        }
    \end{frame}

    \begin{frame}{MNIST}
        \begin{figure}
            \includestandalone[height=1cm, width=6cm]{./figures/mnist_sequence}
            \caption{Sample digits from the MNIST-database.}
        \end{figure}
        \begin{figure}
            \includestandalone[scale=.6]{./figures/cnn_mnist}
            \caption{Mean convergence of two-layer CNN with a dense classifier.}
        \end{figure}
    \end{frame}

    \begin{frame}{Deep convolutional neural networks}
        \begin{figure}
            \includegraphics[width=0.8\linewidth]{figures/deep_cnn.png}
            \caption{Comparing deep networks with and without convolutional structures on the Google-Street view dataset
                \cite[page 199]{goodfellow2016deep}.}
        \end{figure}
        \note{ \cite{goodfellow2016deep} tells us: \\ 
        Eﬀect of number of parameters. Deeper models tend to perform better.
        This is not merely because the model is larger. This experiment from Goodfellow et al.
        (2014d) shows that increasing the number of parameters in layers of convolutional networks
        without increasing their depth is not nearly as eﬀective at increasing test set performance,
        as illustrated in this ﬁgure.}
    \end{frame}


    \begin{frame}[allowframebreaks]{Literature}
        \printbibliography
    \end{frame}
    
\end{document}
